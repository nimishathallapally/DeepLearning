{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-10T15:07:22.751506Z",
     "iopub.status.busy": "2025-04-10T15:07:22.751257Z",
     "iopub.status.idle": "2025-04-10T15:07:28.116373Z",
     "shell.execute_reply": "2025-04-10T15:07:28.115832Z",
     "shell.execute_reply.started": "2025-04-10T15:07:22.751482Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cf1fa51165c40d6b1cedadc24010693",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/4.78k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08f0b7d11ca34a249a53a7dbdc950286",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train.csv:   0%|          | 0.00/46.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "222de46a9dd1453f884dd034ab867c31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test.csv:   0%|          | 0.00/16.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad060152d551409e85f893587115a65f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "val.csv:   0%|          | 0.00/3.37M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3c5c1f97a7e4772a1cb5cfad27b747c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/12565 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "523e163c1a984bbc937a9673d1d26a66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/4487 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f71237b58953422a9cf0b86a97f9742b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/898 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset  = load_dataset(\"ILSUM/ILSUM-1.0\", \"English\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T15:07:52.711289Z",
     "iopub.status.busy": "2025-04-10T15:07:52.710602Z",
     "iopub.status.idle": "2025-04-10T15:07:52.716076Z",
     "shell.execute_reply": "2025-04-10T15:07:52.715382Z",
     "shell.execute_reply.started": "2025-04-10T15:07:52.711262Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'Article', 'Heading', 'Summary'],\n",
       "        num_rows: 12565\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'Article', 'Heading', 'Summary'],\n",
       "        num_rows: 4487\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'Article', 'Heading', 'Summary'],\n",
       "        num_rows: 898\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T15:07:59.934197Z",
     "iopub.status.busy": "2025-04-10T15:07:59.933728Z",
     "iopub.status.idle": "2025-04-10T15:08:00.731404Z",
     "shell.execute_reply": "2025-04-10T15:08:00.730600Z",
     "shell.execute_reply.started": "2025-04-10T15:07:59.934166Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb6262827b2e4a87b3b810f7cc2dc1f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/12565 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d33dd3c83d9e44a582d3fb5311001c5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/4487 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26a2832ebbf243538b0883fb1b61ea69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/898 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Filter Samples to fit total tokens limits (facebook/bart-base - 1024)\n",
    "# Limiting no of words to 800 itself because 1 word can be divided into multiple tokens sometimes\n",
    "def filterSample(sample):\n",
    "    return len(sample['Article'].split()) < 800 and len(sample['Summary'].split()) < 200\n",
    "\n",
    "dataset = dataset.filter(filterSample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T15:08:03.672785Z",
     "iopub.status.busy": "2025-04-10T15:08:03.672116Z",
     "iopub.status.idle": "2025-04-10T15:08:29.870281Z",
     "shell.execute_reply": "2025-04-10T15:08:29.869659Z",
     "shell.execute_reply.started": "2025-04-10T15:08:03.672759Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-10 15:08:05.236015: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744297685.404835      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1744297685.453918      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff2741616eb04c8b885f4057781fcd37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.72k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f076bf2f3414940b0984bfed210d72d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "354398db623e411cb9b08dad78007c49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73060267045842289b1daf70d5d0169b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dd5cdbc59724928bf117183789e8ccc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/558M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1744297708.188287      31 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n",
      "All PyTorch model weights were used when initializing TFBartForConditionalGeneration.\n",
      "\n",
      "All the weights of TFBartForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBartForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer and model\n",
    "\n",
    "import tensorflow as tf\n",
    "from transformers import TFAutoModelForSeq2SeqLM, AutoTokenizer, DataCollatorForSeq2Seq\n",
    "\n",
    "modelCheckpoint = \"facebook/bart-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(modelCheckpoint)      # Tokenizing the input using prerained model\n",
    "model = TFAutoModelForSeq2SeqLM.from_pretrained(modelCheckpoint)  # TensorFlow version of the facebook/bart-base model for sequence-to-sequence learning (Seq2SeqLM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T15:08:36.360620Z",
     "iopub.status.busy": "2025-04-10T15:08:36.359807Z",
     "iopub.status.idle": "2025-04-10T15:08:53.838753Z",
     "shell.execute_reply": "2025-04-10T15:08:53.838133Z",
     "shell.execute_reply.started": "2025-04-10T15:08:36.360582Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecc2592a096044b89cf1fba2758f8d4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10709 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee3cc56103bd45ccb18f85476a68061b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3789 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "480fd7e506d44ea084641dcf2b13b217",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/770 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "maxInputLen = 1024\n",
    "maxTargetLen = 128\n",
    "\n",
    "def preprocessInput(sample):\n",
    "    # pretrained tokenizer ( convert text to numerical token IDs)\n",
    "    modelInputs = tokenizer(sample['Article'], max_length = maxInputLen, padding = \"max_length\", truncation = True)\n",
    "    # Tokenizes the summary\n",
    "    labels = tokenizer(sample[\"Summary\"], max_length = maxTargetLen, padding = \"max_length\", truncation = True)\n",
    "    modelInputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return modelInputs\n",
    "\n",
    "tokenizedDataset = dataset.map(preprocessInput, batched = True, remove_columns = [\"id\", \"Heading\", \"Article\", \"Summary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T15:08:53.840056Z",
     "iopub.status.busy": "2025-04-10T15:08:53.839811Z",
     "iopub.status.idle": "2025-04-10T15:08:53.844113Z",
     "shell.execute_reply": "2025-04-10T15:08:53.843411Z",
     "shell.execute_reply.started": "2025-04-10T15:08:53.840036Z"
    }
   },
   "outputs": [],
   "source": [
    "# Automatically batches, pads, and formats examples\n",
    "#Prepares them as TensorFlow (or PyTorch) model-ready inputs\n",
    "dataCollator = DataCollatorForSeq2Seq(tokenizer = tokenizer, model = model, return_tensors = \"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T15:08:53.844991Z",
     "iopub.status.busy": "2025-04-10T15:08:53.844772Z",
     "iopub.status.idle": "2025-04-10T15:08:55.407178Z",
     "shell.execute_reply": "2025-04-10T15:08:55.406417Z",
     "shell.execute_reply.started": "2025-04-10T15:08:53.844973Z"
    }
   },
   "outputs": [],
   "source": [
    "tfTrain = tokenizedDataset[\"train\"].to_tf_dataset(\n",
    "    columns = [\"input_ids\", \"attention_mask\", \"labels\"],\n",
    "    shuffle = True,\n",
    "    batch_size = 2,\n",
    "    collate_fn = dataCollator\n",
    ")\n",
    "\n",
    "tfVal = tokenizedDataset[\"validation\"].to_tf_dataset(\n",
    "    columns = [\"input_ids\", \"attention_mask\", \"labels\"],\n",
    "    shuffle = False,\n",
    "    batch_size = 2,\n",
    "    collate_fn = dataCollator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T15:08:55.409066Z",
     "iopub.status.busy": "2025-04-10T15:08:55.408761Z",
     "iopub.status.idle": "2025-04-10T15:08:56.201578Z",
     "shell.execute_reply": "2025-04-10T15:08:56.200974Z",
     "shell.execute_reply.started": "2025-04-10T15:08:55.409048Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compilation\n",
    "from transformers import create_optimizer\n",
    "\n",
    "optimizer, schedule = create_optimizer(init_lr = 2e-5, num_warmup_steps = 0, num_train_steps = 1000)\n",
    "model.compile(optimizer = optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T15:08:56.202503Z",
     "iopub.status.busy": "2025-04-10T15:08:56.202294Z",
     "iopub.status.idle": "2025-04-10T16:26:33.712768Z",
     "shell.execute_reply": "2025-04-10T16:26:33.712171Z",
     "shell.execute_reply.started": "2025-04-10T15:08:56.202476Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1744297797.477979     116 service.cc:148] XLA service 0x791bf60db1f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1744297797.478740     116 service.cc:156]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n",
      "I0000 00:00:1744297797.546216     116 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "I0000 00:00:1744297797.663836     116 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5355/5355 [==============================] - 1609s 286ms/step - loss: 0.4159 - val_loss: 0.3501\n",
      "Epoch 2/3\n",
      "5355/5355 [==============================] - 1524s 285ms/step - loss: 0.3950 - val_loss: 0.3501\n",
      "Epoch 3/3\n",
      "5355/5355 [==============================] - 1524s 285ms/step - loss: 0.3964 - val_loss: 0.3501\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf_keras.src.callbacks.History at 0x791c4bc18550>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fine tune the model\n",
    "model.fit(tfTrain, validation_data = tfVal, epochs = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T16:26:33.714796Z",
     "iopub.status.busy": "2025-04-10T16:26:33.714383Z",
     "iopub.status.idle": "2025-04-10T16:26:35.439731Z",
     "shell.execute_reply": "2025-04-10T16:26:35.438996Z",
     "shell.execute_reply.started": "2025-04-10T16:26:33.714778Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/configuration_utils.py:393: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./tf-bart-ilsum/tokenizer_config.json',\n",
       " './tf-bart-ilsum/special_tokens_map.json',\n",
       " './tf-bart-ilsum/vocab.json',\n",
       " './tf-bart-ilsum/merges.txt',\n",
       " './tf-bart-ilsum/added_tokens.json',\n",
       " './tf-bart-ilsum/tokenizer.json')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"./tf-bart-ilsum\")\n",
    "tokenizer.save_pretrained(\"./tf-bart-ilsum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ROUGE( Recall-Oriented Understudy for Gisting Evaluation) evaluation — it’s the most widely used metric for evaluating text summarization tasks.\n",
    "* ROUGE measures overlap between generated and reference summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T17:14:19.928166Z",
     "iopub.status.busy": "2025-04-10T17:14:19.927636Z",
     "iopub.status.idle": "2025-04-10T17:14:25.006511Z",
     "shell.execute_reply": "2025-04-10T17:14:25.005900Z",
     "shell.execute_reply.started": "2025-04-10T17:14:19.928142Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ab96e321d6349d8b61263ab6bc511cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ROUGE Evaluation (on validation set)\n",
    "import evaluate\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T17:14:25.008300Z",
     "iopub.status.busy": "2025-04-10T17:14:25.007681Z",
     "iopub.status.idle": "2025-04-10T17:14:25.012625Z",
     "shell.execute_reply": "2025-04-10T17:14:25.011899Z",
     "shell.execute_reply.started": "2025-04-10T17:14:25.008267Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_summary(example):\n",
    "    input_ids = tokenizer(example[\"Article\"], return_tensors=\"tf\", max_length=1024, truncation=True).input_ids\n",
    "    output_ids = model.generate(input_ids, max_length=128, num_beams=4, early_stopping=True)\n",
    "    summary = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    return {\"Generated Summary\": summary}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T17:14:29.426637Z",
     "iopub.status.busy": "2025-04-10T17:14:29.425898Z",
     "iopub.status.idle": "2025-04-10T17:24:19.592265Z",
     "shell.execute_reply": "2025-04-10T17:24:19.591485Z",
     "shell.execute_reply.started": "2025-04-10T17:14:29.426611Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 0.5288508848630459, 'rouge2': 0.4172175906247514, 'rougeL': 0.4865132082229662, 'rougeLsum': 0.4890689951063759}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on small subset\n",
    "val_subset = dataset[\"validation\"].select(range(50))\n",
    "preds = []\n",
    "refs = []\n",
    "\n",
    "for ex in val_subset:\n",
    "    result = generate_summary(ex)\n",
    "    preds.append(result[\"Generated Summary\"])\n",
    "    refs.append(ex[\"Summary\"])\n",
    "\n",
    "scores = rouge.compute(predictions=preds, references=refs)\n",
    "print(scores)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
